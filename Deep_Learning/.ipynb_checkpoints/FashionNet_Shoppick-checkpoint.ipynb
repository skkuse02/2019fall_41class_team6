{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning algorithm for Shoppick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is taken from [Deep Fashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d5711b125f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import operator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time, pickle, pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D ,Conv2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.applications import VGG16\n",
    "from keras import backend\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import applications\n",
    "from keras.utils.conv_utils import convert_kernel\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the no of classes for the category\n",
    "nb_classes = 46\n",
    "class_name = {\n",
    "    0: 'Anorak',\n",
    "    1: 'Blazer',\n",
    "    2: 'Blouse',\n",
    "    3: 'Bomber',\n",
    "    4: 'Button-Down',\n",
    "    5: 'Caftan',\n",
    "    6: 'Capris',\n",
    "    7: 'Cardigan',\n",
    "    8: 'Chinos',\n",
    "    9: 'Coat',\n",
    "    10: 'Coverup',\n",
    "    11: 'Culottes',\n",
    "    12: 'Cutoffs',\n",
    "    13: 'Dress',\n",
    "    14: 'Flannel',\n",
    "    15: 'Gauchos',\n",
    "    16: 'Halter',\n",
    "    17: 'Henley',\n",
    "    18: 'Hoodie',\n",
    "    19: 'Jacket',\n",
    "    20: 'Jeans',\n",
    "    21: 'Jeggings',\n",
    "    22: 'Jersey',\n",
    "    23: 'Jodhpurs',\n",
    "    24: 'Joggers',\n",
    "    25: 'Jumpsuit',\n",
    "    26: 'Kaftan',\n",
    "    27: 'Kimono',\n",
    "    28: 'Leggings',\n",
    "    29: 'Onesie',\n",
    "    30: 'Parka',\n",
    "    31: 'Peacoat',\n",
    "    32: 'Poncho',\n",
    "    33: 'Robe',\n",
    "    34: 'Romper',\n",
    "    35: 'Sarong',\n",
    "    36: 'Shorts',\n",
    "    37: 'Skirt',\n",
    "    38: 'Sweater',\n",
    "    39: 'Sweatpants',\n",
    "    40: 'Sweatshorts',\n",
    "    41: 'Tank',\n",
    "    42: 'Tee',\n",
    "    43: 'Top',\n",
    "    44: 'Trunks',\n",
    "    45: 'Turtleneck'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(X, y, prediction=-1):\n",
    "    im = X\n",
    "    plt.imshow(im)\n",
    "    if prediction >= 0:\n",
    "        plt.title(\"Class = %s, Predict = %s\" % (class_name[y], class_name[prediction]))\n",
    "    else:\n",
    "        plt.title(\"Class = %s\" % (class_name[y]))\n",
    "\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = './img/train'\n",
    "validation_data_dir = './img/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255        \n",
    "        )\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=50,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=50,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 209215\n",
    "nb_validation_samples = 39999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "for X_batch, Y_batch in train_generator:\n",
    "    for i in range(5):\n",
    "        show_sample(X_batch[i, :, :, :], np.argmax(Y_batch[i]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16(framework='tf'):\n",
    "\n",
    "    if framework == 'th':\n",
    "        # build the VGG16 network in Theano weight ordering mode\n",
    "        #backend.set_image_dim_ordering('th')\n",
    "         backend.set_image_dim_ordering('th')        \n",
    "    else:\n",
    "        # build the VGG16 network in Tensorflow weight ordering mode\n",
    "         backend.set_image_dim_ordering('tf')\n",
    "        \n",
    "    model = Sequential()\n",
    "    if framework == 'th':\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "    else:\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "        \n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "weights_path = 'vgg16_weights(2).h5'\n",
    "th_model = build_vgg16('th')\n",
    "\n",
    "assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "f = h5py.File(weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(th_model.layers):\n",
    "        # we don't look at the last (fully-connected) layers in the savefile\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    \n",
    "#     th_model.layers[k].set_weights(weights)\n",
    "f.close()\n",
    "print('Model loaded.')\n",
    "\n",
    "tf_model = build_vgg16('tf')\n",
    "\n",
    "for th_layer, tf_layer in zip(th_model.layers, tf_model.layers):\n",
    "        tf_layer.set_weights(tf_layer.get_weights())\n",
    "    \n",
    "top_model = Sequential()\n",
    "print (Flatten(input_shape=tf_model.output_shape[1:]))\n",
    "top_model.add(Flatten(input_shape=tf_model.output_shape[1:]))\n",
    "top_model.add(Dense(512, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(46, activation='softmax'))\n",
    "print (tf_model.summary())\n",
    "print(top_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "tf_model.add(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freezing the weights of all layers except top\n",
    "\n",
    "for layer in tf_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Using an Adam optimizer with lower learning rate\n",
    "adaml=tf.train.AdamOptimizers(learning_rate=0.001)\n",
    "\n",
    "tf_model.compile(optimizer = adaml, loss = 'categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model for 5 epochs\n",
    "\n",
    "#tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "checkpoint_callback = ModelCheckpoint('./models/vgg_weights_frozen_pattern.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = 5,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose = 1,\n",
    "        initial_epoch = 0,\n",
    "        callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.array([])\n",
    "losses = np.array([])\n",
    "\n",
    "i=0\n",
    "for X_batch, Y_batch in validation_generator:\n",
    "    loss, accuracy = tf_model.evaluate(X_batch, Y_batch, verbose=0)\n",
    "    losses = np.append(losses, loss)\n",
    "    accuracies = np.append(accuracies, accuracy)\n",
    "    i += 1\n",
    "    if i == 20:\n",
    "        break\n",
    "       \n",
    "print(\"Validation: accuracy = %f  ;  loss = %f\" % (np.mean(accuracies), np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreezing the layers and recompiling the model \n",
    "\n",
    "for layer in tf_model.layers[:-4]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "#Using an Adam optimizer with lower learning rate\n",
    "adaml=tf.train.AdamOptimizers(learning_rate=0.001)\n",
    "\n",
    "tf_model.compile(optimizer = adaml, loss = 'categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading weights with the best validation aaccuracy\n",
    "tf_model.load_weights('./models/vgg_weights_frozen_pattern.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the whole network for 5 epochs first\n",
    "checkpoint_callback = ModelCheckpoint('./models/vgg_weights_best_pattern.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = 10,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose = 1,\n",
    "        initial_epoch = 5,\n",
    "        callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.array([])\n",
    "losses = np.array([])\n",
    "\n",
    "i=0\n",
    "for X_batch, Y_batch in validation_generator:\n",
    "    loss, accuracy = tf_model.evaluate(X_batch, Y_batch, verbose=0)\n",
    "    losses = np.append(losses, loss)\n",
    "    accuracies = np.append(accuracies, accuracy)\n",
    "    i += 1\n",
    "    if i == 20:\n",
    "        break\n",
    "\n",
    "print(\"Validation: accuracy = %f  ;  loss = %f\" % (np.mean(accuracies), np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = 13,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose = 1,\n",
    "        initial_epoch = 10,\n",
    "        callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
